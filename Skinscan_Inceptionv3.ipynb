{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRjplmuxw7gO"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the necessary libraries\n",
        "!pip install tensorflow opencv-python-headless\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import cv2\n"
      ],
      "metadata": {
        "id": "ap-1_9AQxWKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Library"
      ],
      "metadata": {
        "id": "9EOobYnhEYL8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# os\n",
        "import os\n",
        "\n",
        "# OrderedDict\n",
        "from collections import OrderedDict\n",
        "\n",
        "# tqdm\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Path\n",
        "from pathlib import Path\n",
        "\n",
        "# random\n",
        "import random\n",
        "\n",
        "# typing\n",
        "from typing import Dict, List\n",
        "\n",
        "# warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ],
      "metadata": {
        "id": "yMJzyfjSxjBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Input Data"
      ],
      "metadata": {
        "id": "CpODJ5-jEb5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "W-poW29gxpPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Set the path to your data directory\n",
        "ROOT_PATH = Path(\"/content/drive/My Drive/Skin_Problems\")\n",
        "\n",
        "# List all subdirectories in the ROOT_PATH\n",
        "subdirectories = [subdir for subdir in ROOT_PATH.iterdir() if subdir.is_dir()]\n",
        "\n",
        "# Initialize an empty list to store paths to all JPG images\n",
        "IMAGE_PATH_LIST = []\n",
        "\n",
        "# Iterate through each subdirectory\n",
        "for subdir in subdirectories:\n",
        "    # Use glob to list all jpg images within the subdirectory\n",
        "    jpg_files = list(subdir.glob(\"*.jpg\"))\n",
        "    # Extend IMAGE_PATH_LIST with paths to jpg files in the subdirectory\n",
        "    IMAGE_PATH_LIST.extend(jpg_files)\n",
        "\n",
        "print(f'Total Images = {len(IMAGE_PATH_LIST)}')"
      ],
      "metadata": {
        "id": "XBJtGM0x7CcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: Print some paths to verify\n",
        "for path in IMAGE_PATH_LIST[:10]:\n",
        "    print(path)"
      ],
      "metadata": {
        "id": "tlm40XzU_7Y-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# number of images per class.\n",
        "classes = os.listdir(ROOT_PATH)\n",
        "classes = sorted(classes)\n",
        "\n",
        "print(\"**\" * 20)\n",
        "print(\" \" * 10, f\"Total Classes = {len(classes)}\")\n",
        "print(\"**\" * 20)\n",
        "\n",
        "for c in classes:\n",
        "    total_images_class = list(Path(os.path.join(ROOT_PATH, c)).glob(\"*.jpg\"))\n",
        "    print(f\"* {c}: {len(total_images_class)} images\")"
      ],
      "metadata": {
        "id": "0I1mSai-__0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We view some images for each class.\n",
        "NUM_IMAGES = 3\n",
        "\n",
        "fig, ax = plt.subplots(nrows = len(classes), ncols = NUM_IMAGES, figsize = (10,15))\n",
        "p = 0\n",
        "for c in classes:\n",
        "    total_images_class = list(Path(os.path.join(ROOT_PATH, c)).glob(\"*.jpg\"))\n",
        "    images_selected = random.choices(total_images_class, k = NUM_IMAGES)\n",
        "\n",
        "    for i,img_path in enumerate(images_selected):\n",
        "        img_bgr = cv2.imread(str(img_path))\n",
        "        img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
        "        ax[p,i].imshow(img_rgb)\n",
        "        ax[p,i].axis(\"off\")\n",
        "        ax[p,i].set_title(f\"Class: {c}\\nShape: {img_rgb.shape}\", fontsize = 8, fontweight = \"bold\", color = \"black\")\n",
        "\n",
        "    p += 1\n",
        "\n",
        "fig.tight_layout()\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "5Rs3O5eSAEVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "Qc_BfUrYBTYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images_path = [None] * len(IMAGE_PATH_LIST)\n",
        "labels = [None] * len(IMAGE_PATH_LIST)\n",
        "\n",
        "for i,image_path in enumerate(IMAGE_PATH_LIST):\n",
        "    images_path[i] = image_path\n",
        "    labels[i] = image_path.parent.stem\n",
        "\n",
        "df_path_and_label = pd.DataFrame({'path':images_path,\n",
        "                                  'label':labels})\n",
        "df_path_and_label.head()"
      ],
      "metadata": {
        "id": "YPMwHrpZBRVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and validation sets\n",
        "train_df, val_df = train_test_split(df_path_and_label, test_size=0.2, stratify=df_path_and_label['label'], random_state=42)\n",
        "\n",
        "print(f'Training set size: {len(train_df)}')\n",
        "print(f'Validation set size: {len(val_df)}')"
      ],
      "metadata": {
        "id": "pZzsBcf1Kagt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure paths are strings\n",
        "df_path_and_label['path'] = df_path_and_label['path'].astype(str)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_df, val_df = train_test_split(df_path_and_label, test_size=0.2, stratify=df_path_and_label['label'], random_state=42)\n",
        "\n",
        "print(f'Training set size: {len(train_df)}')\n",
        "print(f'Validation set size: {len(val_df)}')"
      ],
      "metadata": {
        "id": "zGA5egmOK0_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data augmentation and preprocessing\n",
        "train_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "    train_df,\n",
        "    x_col='path',\n",
        "    y_col='label',\n",
        "    target_size=(299, 299),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_dataframe(\n",
        "    val_df,\n",
        "    x_col='path',\n",
        "    y_col='label',\n",
        "    target_size=(299, 299),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n"
      ],
      "metadata": {
        "id": "1ALM7VYaKnEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the InceptionV3 model without the top layer\n",
        "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
        "\n",
        "# Add new top layers\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(len(classes), activation='softmax')(x)\n",
        "\n",
        "# Combine the base model and the new top layers\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Freeze the layers of the base model\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Callbacks\n",
        "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, mode='min')\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=50,\n",
        "    validation_data=val_generator,\n",
        "    callbacks=[checkpoint, early_stopping]\n",
        ")"
      ],
      "metadata": {
        "id": "uFm72ronK87Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best model\n",
        "model.load_weights('best_model.h5')\n",
        "\n",
        "# Evaluate the model\n",
        "val_generator.reset()\n",
        "val_loss, val_acc = model.evaluate(val_generator)\n",
        "print(f'Validation loss: {val_loss}')\n",
        "print(f'Validation accuracy: {val_acc}')\n"
      ],
      "metadata": {
        "id": "qi_PjeWoQ01o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot learning curve\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss/Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Learning Curve')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "X7AfMAPNSUlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict the classes\n",
        "val_generator.reset()\n",
        "preds = model.predict(val_generator)\n",
        "y_pred = np.argmax(preds, axis=1)\n",
        "y_true = val_generator.classes\n",
        "\n"
      ],
      "metadata": {
        "id": "uQebhzrZQ62R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification report\n",
        "print(classification_report(y_true, y_pred, target_names=val_generator.class_indices.keys()))\n"
      ],
      "metadata": {
        "id": "ZkceFIS6RBiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', xticklabels=val_generator.class_indices.keys(), yticklabels=val_generator.class_indices.keys())\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0DiItNi7RHIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#================================================"
      ],
      "metadata": {
        "id": "cOEPuUgtKjd2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "train = 70%\n",
        "valid = 15%\n",
        "test = 15%"
      ],
      "metadata": {
        "id": "X6sbc2C7BfwI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We have to define the mapping of the classes to convert the labels to numbers.\n",
        "label_map = dict(zip(classes, range(0, len(classes))))\n",
        "label_map"
      ],
      "metadata": {
        "id": "xFv9L9MlBkE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define tranformasi otomatis untuk gambar"
      ],
      "metadata": {
        "id": "FLSYCptZDEd9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we define the transformations that we are going to apply.\n",
        "weights = ViT_B_16_Weights.DEFAULT\n",
        "auto_transforms = weights.transforms()\n",
        "auto_transforms"
      ],
      "metadata": {
        "id": "agqWPO1DBm9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Membuat dataset dan data loaders"
      ],
      "metadata": {
        "id": "N82uj3JsDQXY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, df:pd.DataFrame, transforms, label_map:dict):\n",
        "        self.df = df\n",
        "        self.transforms = transforms\n",
        "        self.label_map = label_map\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        df_new = self.df.copy()\n",
        "        df_new = df_new.reset_index(drop = True)\n",
        "        df_new[\"label\"] = df_new[\"label\"].map(self.label_map)\n",
        "        image_path = df_new.iloc[idx, 0]\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        image = self.transforms(image)\n",
        "        label = df_new.iloc[idx, 1]\n",
        "\n",
        "        return image,label"
      ],
      "metadata": {
        "id": "xOtM9cdcBpym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create datasets\n",
        "train_dataset = CustomDataset(df_train, auto_transforms, label_map)\n",
        "valid_dataset = CustomDataset(df_val, auto_transforms, label_map)\n",
        "test_dataset = CustomDataset(df_test, auto_transforms, label_map)\n"
      ],
      "metadata": {
        "id": "HQe4jBx3CA5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "-4UN3j1eCFC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data loader"
      ],
      "metadata": {
        "id": "o5bX1dnYegLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 16\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "\n",
        "train_dataloader = DataLoader(dataset = train_dataset,\n",
        "                              batch_size = BATCH_SIZE,\n",
        "                              shuffle = True,\n",
        "                              num_workers = NUM_WORKERS)\n",
        "valid_dataloader = DataLoader(dataset = valid_dataset,\n",
        "                              batch_size = BATCH_SIZE,\n",
        "                              shuffle = True,\n",
        "                              num_workers = NUM_WORKERS)"
      ],
      "metadata": {
        "id": "h8grWIYxeXJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's visualize the dimensions of a batch.\n",
        "batch_images, batch_labels = next(iter(train_dataloader))\n",
        "\n",
        "batch_images.shape, batch_labels.shape"
      ],
      "metadata": {
        "id": "ZjE4IObieexL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "8bkBT_RYeo56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "d_nNtqI3enT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We define the model to use with the pre-trained weights.\n",
        "model = vit_b_16(weights = weights)\n"
      ],
      "metadata": {
        "id": "z9nZPSzveuV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's visualize the architecture of the model.\n",
        "summary(model = model,\n",
        "        input_size = [1, 3, 224, 224],\n",
        "        col_names = [\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width = 15,\n",
        "        row_settings = [\"var_names\"])"
      ],
      "metadata": {
        "id": "-CG-4HwAey1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "freeze the parameters of the conv_proj and encoder layers (ini buat nyegah overfitting sama nyepetin pelatihan gt kubaca)"
      ],
      "metadata": {
        "id": "5iO4FD5-fqIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.conv_proj.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "HlpBUpeKe2u3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.encoder.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "KIKiWbome6v3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see if the parameters were frozen.\n",
        "summary(model = model,\n",
        "        input_size = [1,3,224,224],\n",
        "        col_names = [\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width = 15,\n",
        "        row_settings = [\"var_names\"])"
      ],
      "metadata": {
        "id": "_08Vt3W1e9eH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "parameters were frozen.\n",
        "\n",
        "Let's visualize the last layer which we will modify the number of out_features, in this case it is the number of classes we have."
      ],
      "metadata": {
        "id": "0lRaE18agVBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_shape = len(classes)\n",
        "\n",
        "model.heads = nn.Sequential(OrderedDict([('head', nn.Linear(in_features = 768,\n",
        "                                                            out_features = output_shape))]))"
      ],
      "metadata": {
        "id": "hNxVZiaje_nt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One last time let's take a look if the last layer was modified.\n",
        "summary(model = model,\n",
        "        input_size = [1,3,224,224],\n",
        "        col_names = [\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width = 15,\n",
        "        row_settings = [\"var_names\"])"
      ],
      "metadata": {
        "id": "jn2R97exgdAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "define the loss function and the optimizer."
      ],
      "metadata": {
        "id": "9XjBp2m3ghg0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.01)"
      ],
      "metadata": {
        "id": "vLOfEaNUgerD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "define 3 functions for training and one to store the best model:\n",
        "\n",
        "1. train_step\n",
        "2. save_checkpoint\n",
        "3. valid_step\n",
        "4. train"
      ],
      "metadata": {
        "id": "M5IcVUKTgrOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(model:torch.nn.Module,\n",
        "               dataloader:torch.utils.data.DataLoader,\n",
        "               loss_fn:torch.nn.Module,\n",
        "               optimizer:torch.optim.Optimizer):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    train_loss = 0.\n",
        "    train_accuracy = 0.\n",
        "\n",
        "    for batch,(X,y) in enumerate(dataloader):\n",
        "        X,y = X.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        y_pred_logit = model(X)\n",
        "        loss = loss_fn(y_pred_logit, y)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        y_pred_prob = torch.softmax(y_pred_logit, dim = 1)\n",
        "        y_pred_class = torch.argmax(y_pred_prob, dim = 1)\n",
        "        train_accuracy += accuracy_score(y.cpu().numpy(),\n",
        "                                         y_pred_class.detach().cpu().numpy())\n",
        "\n",
        "    train_loss = train_loss/len(dataloader)\n",
        "    train_accuracy = train_accuracy/len(dataloader)\n",
        "\n",
        "    return train_loss, train_accuracy"
      ],
      "metadata": {
        "id": "GqAHHsOugmrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(filename, model, loss, epoch, optimizer, metric):\n",
        "    state = {\"filename\":filename,\n",
        "             \"model\":model.state_dict(),\n",
        "             \"loss\":loss,\n",
        "             \"epoch\":epoch,\n",
        "             \"optimizer\":optimizer.state_dict(),\n",
        "             \"metric\":metric}\n",
        "\n",
        "    torch.save(state, filename)"
      ],
      "metadata": {
        "id": "dx_a1yqbg-qa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def valid_step(model:torch.nn.Module,\n",
        "               dataloader:torch.utils.data.DataLoader,\n",
        "               loss_fn:torch.nn.Module):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    valid_loss = 0.\n",
        "    valid_accuracy = 0.\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        for batch,(X,y) in enumerate(dataloader):\n",
        "            X,y = X.to(device), y.to(device)\n",
        "            y_pred_logit = model(X)\n",
        "            loss = loss_fn(y_pred_logit, y)\n",
        "            valid_loss += loss.item()\n",
        "\n",
        "            y_pred_prob = torch.softmax(y_pred_logit, dim = 1)\n",
        "            y_pred_class = torch.argmax(y_pred_prob, dim = 1)\n",
        "\n",
        "            valid_accuracy += accuracy_score(y.cpu().numpy(), y_pred_class.detach().cpu().numpy())\n",
        "            valid_loss = valid_loss/len(dataloader)\n",
        "    valid_accuracy = valid_accuracy/len(dataloader)\n",
        "\n",
        "    return valid_loss, valid_accuracy"
      ],
      "metadata": {
        "id": "cqEXx6WJhGpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model:torch.nn.Module,\n",
        "          train_dataloader:torch.utils.data.DataLoader,\n",
        "          valid_dataloader:torch.utils.data.DataLoader,\n",
        "          loss_fn:torch.nn.Module,\n",
        "          optimizer:torch.optim.Optimizer,\n",
        "          epochs:int = 10):\n",
        "\n",
        "    results = {\"train_loss\":[],\n",
        "               \"train_accuracy\":[],\n",
        "               \"valid_loss\":[],\n",
        "               \"valid_accuracy\":[]}\n",
        "\n",
        "    best_valid_loss = float(\"inf\")\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        train_loss, train_accuracy = train_step(model = model,\n",
        "                                                dataloader = train_dataloader,\n",
        "                                                loss_fn = loss_fn,\n",
        "                                                optimizer = optimizer)\n",
        "        valid_loss, valid_accuracy = valid_step(model = model,\n",
        "                                                dataloader = valid_dataloader,\n",
        "                                                loss_fn = loss_fn)\n",
        "\n",
        "        if valid_loss < best_valid_loss:\n",
        "            best_valid_loss = valid_loss\n",
        "            file_name = \"best_model.pth\"\n",
        "            save_checkpoint(file_name, model, best_valid_loss, epoch, optimizer, valid_accuracy)\n",
        "\n",
        "        print(f\"Epoch: {epoch + 1} | \",\n",
        "              f\"Train Loss: {train_loss:.4f} | \",\n",
        "              f\"Train Accuracy: {train_accuracy:.4f} | \",\n",
        "              f\"Valid Loss: {valid_loss:.4f} | \",\n",
        "              f\"Valid Accuracy: {valid_accuracy:.4f}\")\n",
        "\n",
        "        results[\"train_loss\"].append(train_loss)\n",
        "        results[\"train_accuracy\"].append(train_accuracy)\n",
        "        results[\"valid_loss\"].append(valid_loss)\n",
        "        results[\"valid_accuracy\"].append(valid_accuracy)\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "qlgNDV7ThkGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training!!!\n",
        "EPOCHS = 80\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "MODEL_RESULTS = train(model,\n",
        "                      train_dataloader,\n",
        "                      valid_dataloader,\n",
        "                      loss_fn,\n",
        "                      optimizer,\n",
        "                      EPOCHS)\n"
      ],
      "metadata": {
        "id": "HWXHOCOPhrac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to plot the loss and metric during each training epoch.\n",
        "def loss_metric_curve_plot(model_results:Dict[str,List[float]]):\n",
        "\n",
        "    train_loss = model_results[\"train_loss\"]\n",
        "    valid_loss = model_results[\"valid_loss\"]\n",
        "\n",
        "    train_accuracy = [float(value) for value in model_results[\"train_accuracy\"]]\n",
        "    valid_accuracy = [float(value) for value in model_results[\"valid_accuracy\"]]\n",
        "\n",
        "    fig,axes = plt.subplots(nrows = 1, ncols = 2, figsize = (10,4))\n",
        "    axes = axes.flat\n",
        "\n",
        "    axes[0].plot(train_loss, color = \"red\", label = \"Train\")\n",
        "    axes[0].plot(valid_loss, color = \"blue\", label = \"Valid\")\n",
        "    axes[0].set_title(\"CrossEntropyLoss\", fontsize = 12, fontweight = \"bold\", color = \"black\")\n",
        "    axes[0].set_xlabel(\"Epochs\", fontsize = 10, fontweight = \"bold\", color = \"black\")\n",
        "    axes[0].set_ylabel(\"Loss\", fontsize = 10, fontweight = \"bold\", color = \"black\")\n",
        "    axes[0].legend()\n",
        "\n",
        "    axes[1].plot(train_accuracy, color = \"red\", label = \"Train\")\n",
        "    axes[1].plot(valid_accuracy, color = \"blue\", label = \"Valid\")\n",
        "    axes[1].set_title(\"Metric of performance: Accuracy\", fontsize = 12, fontweight = \"bold\", color = \"black\")\n",
        "    axes[1].set_xlabel(\"Epochs\", fontsize = 10, fontweight = \"bold\", color = \"black\")\n",
        "    axes[1].set_ylabel(\"Score\", fontsize = 10, fontweight = \"bold\", color = \"black\")\n",
        "    axes[1].legend()\n",
        "\n",
        "    fig.tight_layout()\n",
        "    fig.show()"
      ],
      "metadata": {
        "id": "fT2YaHwvWmm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_metric_curve_plot(MODEL_RESULTS)"
      ],
      "metadata": {
        "id": "n0PZGp_iWxv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mengubah jalur file untuk Google Colab\n",
        "checkpoint_path = \"/content/best_model.pth\"\n",
        "checkpoint = torch.load(checkpoint_path)"
      ],
      "metadata": {
        "id": "3aua2BpvW0MO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's look at the smallest loss, its metric and when it occurred.\n",
        "print(f'Best Loss: {checkpoint[\"loss\"]}')\n",
        "print(f'Epoch: {checkpoint[\"epoch\"] + 1}')\n",
        "print(f'Best Metric: {checkpoint[\"metric\"]}')"
      ],
      "metadata": {
        "id": "b4qOpiFxaDrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction"
      ],
      "metadata": {
        "id": "E23fbi0uaJaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First of all, we create the Dataset, DataLoader\n",
        "test_dataset = CustomDataset(df_test, auto_transforms, label_map)\n",
        "test_dataloader = DataLoader(dataset = test_dataset, shuffle = False, num_workers = NUM_WORKERS)"
      ],
      "metadata": {
        "id": "1clJyWLsaF2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We define the model again with its respective modification.\n",
        "loaded_model = vit_b_16()\n",
        "\n",
        "loaded_model.heads = nn.Sequential(OrderedDict([('head',nn.Linear(in_features = 768,\n",
        "                                                                  out_features = output_shape))]))\n",
        "\n",
        "loaded_model.load_state_dict(checkpoint[\"model\"])\n",
        "\n",
        "# We now infer\n",
        "loaded_model.to(device)\n",
        "\n",
        "loaded_model.eval()\n",
        "\n",
        "y_pred_test = []\n",
        "\n",
        "with torch.inference_mode():\n",
        "    for X,y in tqdm(test_dataloader):\n",
        "        X,y = X.to(device), y.to(device)\n",
        "        y_pred_logit = loaded_model(X)\n",
        "        y_pred_prob = torch.softmax(y_pred_logit, dim = 1)\n",
        "        y_pred_class = torch.argmax(y_pred_prob, dim = 1)\n",
        "        y_pred_test.append(y_pred_class.detach().cpu())"
      ],
      "metadata": {
        "id": "cgh6g61laLsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_test = torch.cat(y_pred_test).numpy()"
      ],
      "metadata": {
        "id": "4Uh06g65aU6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metrics"
      ],
      "metadata": {
        "id": "XgrCX4mYaWfE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Acuracy"
      ],
      "metadata": {
        "id": "M2sO_R7xaZQy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Accuracy = {round(accuracy_score(df_test[\"label\"].map(label_map), y_pred_test), 4)}')"
      ],
      "metadata": {
        "id": "v0zQ6QZMaYAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confusion Matrix"
      ],
      "metadata": {
        "id": "vhf2tsj2afwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix_test = confusion_matrix(df_test[\"label\"].map(label_map), y_pred_test)"
      ],
      "metadata": {
        "id": "WoOXizxlad19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig,ax = plt.subplots(figsize = (15,4))\n",
        "sns.heatmap(confusion_matrix_test,\n",
        "            cmap = 'coolwarm',\n",
        "            annot = True,\n",
        "            annot_kws = {\"fontsize\":9, \"fontweight\":\"bold\"},\n",
        "            linewidths = 1.2,\n",
        "            linecolor = \"black\",\n",
        "            square = True,\n",
        "            xticklabels = classes,\n",
        "            yticklabels = classes,\n",
        "            cbar = False,\n",
        "            ax = ax)\n",
        "ax.set_title(\"Confusion Matrix Test\", fontsize = 10, fontweight = \"bold\", color = \"darkblue\")\n",
        "ax.tick_params('x',rotation = 90)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "KvSqj9jzajGc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}